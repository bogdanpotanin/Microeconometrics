# --------
# Потанин Богдан Станиславович
# Микроэконометрика в R :)
# Урок 1. Метод максимального правдоподобия
# --------

# Отключим scientific notation
options(scipen = 999)

#---------------------------------------------------
# Часть 1. Получение реализаций оценок метода
#          максимального правдоподобия аналитическим
#          и численным методами
#---------------------------------------------------

# Симулируем выборку X = (X1,...,Xn) из 
# нормального распределения N(mu, sigma ^ 2)
set.seed(123)                              # в целях воспроизводимости результатов

n <- 10000                                 # объем выборки: чем он больше, 
                                           # тем выше эффективность ММП оценок
mu <- 2                                    # математическое ожидание
sigma <- 5                                 # стандартное отклонение
X <- rnorm(n = n,                          # симулируем выборку объема n 
                                           # из нормального распределения
           mean = mu,                      # с математическим ожиданием mu
           sd = sigma)                     # и стандартным отклонением sigma

hist(X, breaks = 100)                      # построим гистограмму

# Рассчитаем значение логарифма функции
# правдоподобия в точке истинных значений
# параметров распределения
X_den <- dnorm(x = X,                      # для каждой из реализаций выборки
                                           # считаем функцию плотности 
                                           # нормального распределения
               mean = mu,                  # с математическим ожиданием mu
               sd = sigma)                 # и стандартным отклонением sigma
X_den_log <- log(X_den)                    # считаем логарифмы
data.frame("Observations" = X,             # наблюдения
           "Densities" = X_den,            # функция плотности посчитанная по наблюдениям
           "Log Densities" = X_den_log)    # и ее логарифм
lnL_value <- sum(X_den_log)                # получаем логарифм функции правдоподобия
                                           # по результатам сложения значений логарифмов
                                           # функций плотности, рассчитанных по выборке

# Поскольку на практике значения mu и
# sigma нам неизвестны, то их необходимо
# оценить, для чего мы воспользуемся
# методом максимально правдоподобия

# В качестве тренировки перед созданием
# функции правдоподобия напишем функцию, 
# которая будет работать так же, 
# как dnorm()
dn <- function(x, mean, sd)
{
  value <- 1 / (sqrt(2 * pi) * sd) *      # осуществляем расчет по формуле
           exp(-(x - mean) ^ 2 /          # функции плотности нормального
                (2 * sd ^ 2))             # распределения с математическим
                                          # mean и стандартным отклонением 
                                          # sd в каждой из точек вектора x
  return(value)
}
# сравним результаты
dnorm(x = 1, mean = 2, sd = 3)            # встроенная функция в одной точке
dn(x = 1, mean = 2, sd = 3)               # наша функция в одной точке
dnorm(x = c(1, 4, 5), mean = 2, sd = 3)   # встроенная функция в нескольких точках
dn(x = c(1, 4, 5), mean = 2, sd = 3)      # наша функция в нескольких точках

# Запрограммируем функцию для расчета 
# логарифма функции правдоподобия 
# нормального распределения 
# при заданной реализации выборки:
lnL <- function (x,                        # вектор параметров нормального распределения
                 X,                        # реализация выборки
                 is_aggregate = TRUE)      # возвращаем сумму вкладов наблюдений в функцию  
                                           # правдоподобия (TRUE) или по отдельности (FALSE)
{                                         
  mu <- x[1]                               # для удобства создадим отдельные переменные под
  sigma <- x[2]                            # значения оптимизируемых параметров
  
  if(sigma <= 0)                           # проверяем, что оптимизатор
  {                                        # не использует отрицательные значения
    return(NA)                             # для стандартного отклонения и в противном
  }                                        # случае возвращаем пропуск
  
  L_vector <- dnorm(x = X,                 # считаем функцию плотности для реализации 
                                           # каждого элемента выборки
                    mean = mu,             # задаем математическое ожидание
                    sd = sigma)            # задаем стандартное отклонение 
                                           # как корень из дисперсии
  
  lnL_value <- log(L_vector)               # считаем значение логарифма 
                                           # функции правдоподобия
  
  if(!is_aggregate)                        # возвращаем значение логарифма 
  {                                        # функции правдоподобия при        
    return(lnL_value)                      # заданных значениях параметров
  }
  
  return(sum(lnL_value))                   # возвращаем значения для каждого
                                           # наблюдения по отдельности
}

# Посчитаем функцию правдоподобия
lnL(x = c(mu, sigma), X = X)               # с использованием истинных значений
                                           # параметров распределения
lnL(x = c(10, 10), X = X)                  # в произвольной точке
lnL(x = c(5, 8), X = X,                    # в произвольной точке и вернем
    is_aggregate = FALSE)                  # вклады наблюдений в логарифм 
                                           # функции правдоподобия

# Для начала найдем оценки mu и sigma
# используя аналитическую формулу
mu_MLE_0 <- mean(X)
sigma_MLE_0 <- sqrt(var(X) * (n - 1) / n)  # по умолчанию  функция var() считает 
                                           # скорретированное выборочную дисперсию,
                                           # а ММП оценка параметра 
                                           # sigma совпадает с квалратным корнем из
                                           # не скорректированой скорректированной

# Теперь воспользуемся методом
# численной оптимизации для того,
# чтобы найти реализации ММП оценок
opt_MLE <- optim(par = c(0, 1),                     # в качестве начальной точки
                                                    # возьмем параметры стандартного
                                                    # нормального распределения
                 fn = lnL,                          # максимизируемая функция правдоподобия
                 method = "BFGS",                   # оптимизационный алгоритм
                 hessian = TRUE,                    # возвращаем Гессиан, он нам понадобится
                 control = list(maxit = 10000,      # максимальное количество итераций
                                fnscale = -1,       # ставим -1 чтобы сделать задачу максимизационной
                                reltol = 1e-10),    # условие остановки алгоритма (termination condition)
                 X = X)                             # в конце задаем дополнительный аргумент для функции
                                                    # правдоподобия lnL, а именно, выборку
mu_MLE_1 <- opt_MLE$par[1]                          # реализация оценки mu
sigma_MLE_1 <- opt_MLE$par[2]                       # реализация оценки sigma
lnL_value <- opt_MLE$value                          # логарифм функции правдоподобия в точке
                                                    # полученных ММП оценок

# Сравним результаты и убедимся, что 
# они практически не различаются
df <- data.frame("Analytical" = c(mu_MLE_0, sigma_MLE_0), 
                 "Numeric" = c(mu_MLE_1, sigma_MLE_1),
                 "Real" = c(mu, sigma))
rownames(df) <- c("mu_MLE", "sigma_MLE")
print(df)

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 1.1.    Попробуйте найти оценку максимального правдоподобия
#         параметров нормального распределения численно, используя
#         начальную точку mu0 = 1 и sigma0 = 2.
#         Подсказка: измените значение параметра par в optim()
# 1.2.    Измените объем выборки на 5000, а математическое ожидание
#         и дисперсию распределения на 10 и 100 соответственно. Далее,
#         подберите начальную точку mu0 и sigma0, после чего оцените
#         параметры mu и sigma с помощью метода максимального 
#         правдоподобия, используя численную оптимизацию.
# 1.3.    Симулируйте выборки объемом 10000 наблюдений
#         и найдите оценки максимального правдоподобия
#         численным методом для следующих распределений:
#         1)    Экспоненциального с математическим ожиданием 5
#               Подсказка: используйте функции rexp() и dexp()
#         2)    Хи-квадрат с 8 степенями свободы
#               Подсказка: используйте функции rchisq() и dchisq()
#         3)    Стьюдента с 10 степенями свободы
#               Подсказка: используйте функции rt() и dt()
#         4)    Пуассона с параметром lambda = 5
#               Подсказка: используйте функции rpois() и dpois()
#         5)    Логистического распределения, у которого параметры
#               формы (shape) и масштаба (scale) равняются 2 и 5
#               соответственно
#               Подсказка: используйте функции rlogis() и dlogis()
#         6)    Гамма распределения, у которого параметры
#               формы (shape) и масштаба (scale) равняются 2 и 5
#               соответственно
#               Подсказка: используйте функции rgamma() и dgamma()
#         7)    Бета распределения, у которого параметры
#               формы (shape) и масштаба (scale) равняются 2 и 5
#               соответственно
#               Подсказка: используйте функции rbeta() и dbeta()
#         8*)   Двумерного нормального распределения, где маржинальное
#               распределение первой компоненты имеет математическое
#               ожидание 2 и дисперсию 5, а у второй компоненты 
#               математическое ожидание равняется 3 и дисперсия 10, при
#               этом корреляция составляет 0.5
#               Подсказка: используйте пакет mvtnorm и входящие
#                          в него функции rmvnorm() и dmvnorm()
# 1.4.    Пусть t1 и t2 - независимые случайные величины, имеющие 
#         распределение Стьюдента с df1 и df2 степенями свободы
#         соответственно. Также, имеется независимая от них бернулиевская
#         случайная величина V, такая, что P(V = 1) = p. Рассмотрим
#         распределение следующей случайной величины:
#         G = V * (t5 - a) + (1 - V) * (t10 + b).
#         Пусть df1 = 5, df2 = 10, a = 2, b = 3, p = 0.5.
#         1*)   Симулируйте выборку объемом 10000 
#               наблюдений из G
#         2**)  Запрограммируйте функцию плотности G
#         3**)  Оцените параметры df1, df2, a, b и p с 
#               помощью метода максимального правдоподобия
#         Подсказка: сперва найдите функцию распределения G как
#                    функцию от функций плотности t5 и t10, а затем
#                    запрограммируйте функцию плотности G и далее
#                    действуйте по аналогии с предыдущими задачами
# 1.5.    Имеется следующая модель: 
#         yi = b0 + x1i ^ b1 + b2 ^ x2i + ei, где
#         i - индекс наблюдения в выборке (от 1 до 10000)
#         y - зависимая переменная
#         x1, x2 - независимые переменные
#         e - i.i.d случайные ошибки из распределения
#             Стьюдедента с df степенями свободы, которые
#             не зависит от распределения x1 и x2
#         b0, b1, b2, df - оцениваемые параметры.
#         Пусть b0 = 0.5, b1 = 0.75, b2 = 0.9, df = 10:
#         1*)   Симулируйте x1 и x2 из модуля двумерного нормального
#               распределения с нулевым вектором математических ожиданий,
#               единичными дисперсиями и корреляцией 0.5. Затем, симулируйте
#               yi и ei в соответствии с указанным выше процессом генерации
#               данных.
#               Подсказка: используйте функцию rmvnorm()
#         2**)  Оцените параметры b0, b1, b2 и df при помощи 
#               метода максимального правдоподобия

#---------------------------------------------------
# Часть 2. Расчет реализаций оценок ковариационной 
#          матрицы оценок метода максимального 
#          правдоподобия с использование различных 
#          подходов и построение асимптотических
#          доверительных интервалов
#---------------------------------------------------

# Асимптотическое распределение вектора ММП оценок 
# является многомерным нормальным. Чтобы тестировать
# гипотезы об оцениваемых параметрах и строить
# асимптотические доверительные интервалы, необходимо
# оценить параметры данного распределения. Поскольку
# асимптотическое распределение многомерное нормальное, то
# его параметры будут совпадать с асимптотическим математическим 
# ожиданием и асимптотической ковариационной матрицей вектора ММП 
# оценок. Ввиду того, что ММП оценки состоятельны, асимптотическое
# математическое ожидание совпадает с истинным значением
# оцениваемых параметров. Поэтому, остается найти лишь
# состоятельную оценку асимптотической ковариационной 
# матрицы ММП оценок.

library("numDeriv")                                   # подключим библиотеку для
                                                      # численного дифференцирования

# Найдем реализацию оценки асимптотической
# ковариационной матрицы, используя:

  # 1) аналитическую формулу, опирающуюся 
  #    на информацию Фишера (эффективно)
cov_MLE_0 <- matrix(c(sigma_MLE_0 ^ 2 / n, 0,         # осуществляем расчет
                      0, 0.5 * sigma_MLE_0 ^ 2 / n), 
                    ncol = 2)
rownames(cov_MLE_0) <- c("mu_MLE", "sigma_MLE")       # даем имена строкам и
colnames(cov_MLE_0) <- c("mu_MLE", "sigma_MLE")       # столбцам матрицы
print(cov_MLE_0)                                      # смотрим результат

  # 2) альтернативную формулу, опирающуюся на
  #    матрицу, обратную Гессиану логарифма
  #    функции правдоподобия (удобно)
lnL_hessian <- hessian(func = lnL,                    # рассчитываем Гессиан
                       x = c(mu_MLE_1, sigma_MLE_1),  # численным методом
                       X = X)                         # в точке реализации ММП оценок
fisher_est <- (-1) * lnL_hessian                      # получаем реализацию оценки 
                                                      # информации Фишера
cov_MLE_1 <- solve(fisher_est)                        # считаем реализацию оценки 
                                                      # ковариационной матрицы оценок
rownames(cov_MLE_1) <- c("mu_MLE", "sigma_MLE")       # даем имена строкам и
colnames(cov_MLE_1) <- c("mu_MLE", "sigma_MLE")       # столбцам матрицы
print(cov_MLE_1)                                      # смотрим результат

  # 3) бутстрап, позволяющий симулировать выборку
  #    из приблизительно такого же распределения,
  #    как и у ММП оценок и за счет этого оценить
  #    ковариационную матрицу вектора ММП оценок
boot_iter <- 1000                                     # число итераций бутстрапа
boot_est <- data.frame("mu_MLE" = rep(NA, boot_iter), # каждую итерацию складываем оценки
                       "sigma_MLE" = rep(NA,          # в соответствующие строки датафрейма
                                        boot_iter))   
for(i in 1:boot_iter)
{
  X_boot <- sample(x = X,                             # из выборки X формируем новую выборку
                   size = n,                          # такого же объема как X, используя выбор
                   replace = TRUE)                    # с возвращением (наблюдения могут повторяться)
  boot_est$mu_MLE[i] <- mean(X_boot)                  # для быстроты считаем оценки аналитически, но
  boot_est$sigma_MLE[i] <- sqrt(var(X_boot)  *        # можно было и численно через optim()
                                (n - 1) / n)  
}
cov_MLE_boot <- cov(boot_est)                         # осуществляем расчет
rownames(cov_MLE_boot) <- c("mu_MLE", "sigma_MLE")    # даем имена строкам и
colnames(cov_MLE_boot) <- c("mu_MLE", "sigma_MLE")    # столбцам матрицы
print(cov_MLE_boot)                                   # смотрим результат

  # 4) произведение Якобианов, именуемое как
  #    Gradient Outer Product (GOP)
J <- jacobian(func = lnL,                             # рассчитаем Якобиан функции, возвращающей
              x = c(mu_MLE_1, sigma_MLE_1),           # логарифмы вкладов наблюдений в функцию
              X = X, is_aggregate = FALSE)            # правдоподобия
cov_MLE_gop <- solve(t(J) %*% J)                      # осуществляем расчет
rownames(cov_MLE_gop) <- c("mu_MLE", "sigma_MLE")     # даем имена строкам и
colnames(cov_MLE_gop) <- c("mu_MLE", "sigma_MLE")     # столбцам матрицы
print(cov_MLE_gop)                                    # смотрим результат

  # 5) Сэндвич - устойчивый к нарушению допущения
  #    о распределении элементов выборки, используется
  #    в методе квази максимального правдоподобия.
H <- lnL_hessian                                      # H - хлеб
                                                      # J - масло
cov_MLE_sandwich <- solve(H) %*%                      # сэндвич
                    t(J) %*% J %*%
                    solve(H)                          
rownames(cov_MLE_sandwich) <- c("mu_MLE",             # даем имена 
                                "sigma_MLE")          # строкам и
colnames(cov_MLE_sandwich) <- c("mu_MLE",             # столбцам матрицы
                                "sigma_MLE")        
print(cov_MLE_sandwich)                               # смотрим результат

  # 6) истинные значения
cov_MLE_true <- matrix(c(sigma ^ 2 / n, 0,            # осуществляем расчет
                         0, 0.5 * sigma ^ 2 / n), 
                       ncol=2)
rownames(cov_MLE_true) <- c("mu_MLE", "sigma_MLE")    # даем имена строкам и
colnames(cov_MLE_true) <- c("mu_MLE", "sigma_MLE")    # столбцам матрицы
print(cov_MLE_true)                                   # смотрим результат

# Сравним полученные результаты
df <- data.frame("Analytical" = diag(cov_MLE_0), 
                 "Numeric" = diag(cov_MLE_1),
                 "Bootstrap" = diag(cov_MLE_boot),
                 "GOP" = diag(cov_MLE_gop),
                 "Sandwich" = diag(cov_MLE_sandwich),
                 "Real" = diag(cov_MLE_true))
rownames(df) <- c("As.Var(mu_MLE)", "As.Var(sigma_MLE)")
print(df)

# Используя полученные оценки построим 95%
# асимптотический доверительный интервал (АДИ) для mu,
# применяя любую из рассчитанных ранее оценок 
# ковариационных матриц
conf <- 0.95                     # указываем уровень доверия
alpha <- 1 - conf
q_L_1 <- mu_MLE_1 -              # считаем левую границу АДИ используя 
         qnorm(1 - alpha / 2) *  # квантиль стандартного
                                 # нормального распределения и          
         sqrt(cov_MLE_1[1, 1])   # корень из оценки асимптотической
                                 # дисперсии ММП оценки mu
q_R_1 <- mu_MLE_1 +              # считаем правую границу АДИ используя 
         qnorm(1 - alpha / 2) *  # квантиль стандартного
                                 # нормального распределения и          
         sqrt(cov_MLE_1[1, 1])   # корень из оценки асимптотической
                                 # дисперсии ММП оценки mu
c(q_L_1, q_R_1)                  # смотрим полученный АДИ

# Построим бутстрапированный 95% 
# доверительный интервал для mu
conf <- 0.95                                      # указываем уровень доверия
alpha <- 1 - conf
q_L_2 <- quantile(boot_est$mu_MLE,                # берем (100 * (alpha / 2))% самых
                  alpha / 2)                      # маленьких реализаций оценок my
q_R_2 <- quantile(boot_est$mu_MLE, 1 - alpha / 2) # берем (100 * (alpha / 2))% самых
                                                  # больших реализаций оценок my
c(q_L_2, q_R_2)

# Сравним результаты получения доверительных
# интервалов классическим способом и при
# помощи бутстрапа
df <- data.frame("Classic" = c(q_L_1, q_R_1),
                 "Bootstrap" = c(q_L_2, q_R_2))
rownames(df) <- c("Left", "Right")
print(df)

# Используя дельта метод построим 90%
# асимптотический доверительный интервал
# для mu ^ 3
conf <- 0.9                                       # указываем уровень доверия
alpha <- 1 - conf
mu_3_est <- mu_MLE_1 ^ 3                          # оценка асимптотического математического
                                                  # ожидания ММП оценки (mu ^ 3)
mu_3_asvar_est <- (3 * mu_MLE_1 ^ 2) ^ 2 *        # оценка асимптотической дисперсии
                  cov_MLE_1[1, 1]                 # ММП оценки (mu ^ 3)
q_L_3 <- mu_3_est -             
         qnorm(1 - alpha / 2) *
         sqrt(mu_3_asvar_est)
q_R_3 <- mu_3_est +             
         qnorm(1 - alpha / 2) *
         sqrt(mu_3_asvar_est)
c(q_L_3, q_R_3)

# Построим бутстрапированный 90% 
# доверительный интервал для mu ^ 3
# при помощи бутстрапа
conf <- 0.9                                           # указываем уровень доверия
alpha <- 1 - conf
q_L_4 <- quantile(boot_est$mu_MLE ^ 3,                # берем (100 * (alpha / 2))% самых
                  alpha / 2)                          # маленьких реализаций оценок my
q_R_4 <- quantile(boot_est$mu_MLE ^ 3, 1 - alpha / 2) # берем (100 * (alpha / 2))% самых
c(q_L_4, q_R_4)
# больших реализаций оценок my
# Сравним результаты получения доверительных
# интервалов классическим способом и при
# помощи бутстрапа
df <- data.frame("Classic" = c(q_L_3, q_R_3),
                 "Bootstrap" = c(q_L_4, q_R_4))
rownames(df) <- c("Left", "Right")
print(df)

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 2.1.    Посчитайте ковариационную матрицу при помощи бутстрапа 
#         используя 10, 100 и 500 итераций. Сравните полученные 
#         результаты и определите, какой из них ближе всего к истине
# 2.2.    Постройте 90% асимптотический доверительный интервал для
#         1)     параметра sigma
#         2*)    функции 2 * (sigma ^ 0.5) от параметра sigma
#         3**)   функции mu / sigma от параметров mu и sigma
# 2.3.    Продолжите задание 1.3, рассчитав для каждого из оцененных
#         параметров распределения:
#         1)     Все рассмотренные выше виды оценок асимптотической
#                ковариационной матрицы ММП оценок
#         2)     90% асимптотической доверительный интервал
#         3*)    Для распределений с несколькими параметрами постройте
#                99% асимптотический доверительный интервал для их суммы
#         4*)    Для распределений с несколькими параметрами постройте
#                99% асимптотический доверительный интервал для их произведения
#         5*)    Для вероятности того, что первое наблюдение в выборке X1
#                превысит истинное математическое ожидание E(X1) более, чем на
#                одно стандартное отклонение Var(X1) ^ 0.5
#                Подсказка: обратите внимание, что данная вероятность будет
#                           некоторой функцией от оцениваемых параметров
# 2.4.    Продолжите задание 1.4. и найдите:
#         1*)    Асимптотическую оценку ковариационной матрицы ММП оценок
#                всеми рассмотренными в данном разделе способами
#         2**)   95% асимптотический доверительный интервал для вероятности
#                того, что G превысит E(G) более, чем на 10%.
# 2.5.    Продолжите задание 1.5. и найдите:
#         1*)    Оценку ковариационной матрицы ММП оценок всеми рассмотренными
#                в данном разделе способами
#         2*)    Асимптотический 95% доверительный интервал для b1 + b2
#         3**)   Асимптотический 95% доверительный интервал для 
#                предельного эффекта x1 на E(y|x1, x2) при x1 = 2 и x2 = 3
#         4**)   Асимптотический 95% доверительный интервал для 
#                предельного эффекта x2 на E(y|x1, x2) при x1 = 2 и x2 = 3
#         5**)   Асимптотический 95% доверительный интервал для 
#                отношения предельных эффектов, посчитанных в предыдущих пунктах

#---------------------------------------------------
# Часть 3. Тестирование гипотез о параметрах 
#          с использованием оценок максимального
#          правдоподобия
#---------------------------------------------------

# Пример №1
# H0: mu = 1.88
mu_H0 <- 1.88                                    # тестируем гипотезу о равенстве mu
                                                 # данному значению
z <- (mu_MLE_1 - mu_H0) / sqrt(cov_MLE_1[1, 1])  # рассчитываем значение тестовой статистики
                                                 # используя любую из ранее посчитанных оценок
                                                 # асимптотических ковариационных матриц
p_value <- 2 * min(pnorm(z), 1 - pnorm(z))       # вычисляем p-value
print(p_value)

# Пример №2
# H0: (sigma ^ 0.5) * exp(mu) = 15

# Создадим функцию:
# g(mu, sigma) = (sigma ^ 0.5) * exp(mu)
g <- function(x)          
{
  mu <- x[1]                                     # достаем параметры
  sigma <- x[2]                                  # из вектора параметров
  
  value <- (sigma ^ 0.5) * exp(mu)               # считаем значение функции
  
  return(value)                                  # возвращаем посчитанный результат
}

# Вычислим градиент данной функции в точке
# реализаций ММП оценок
g_grad_est <- grad(func = g,                     # найдем градиент численно, но
                   x = c(mu_MLE_1, sigma_MLE_1)) # можно было бы и аналитически
                                                  
# Воспользуемся дельта методом и свойством
# инвариантности для нахождения ММП оценки g
# и оценки ее асимптотической ковариационной
# матрицы
g_MLE_est <- g(c(mu_MLE_1, sigma_MLE_1))                 # ММП оценка g
g_cov_est <- t(g_grad_est) %*% cov_MLE_1 %*% g_grad_est  # оценка асимптотической 
                                                         # дисперсии ММП оценки g  

# Осуществим проверку гипотезы
g_H0 <- 15                                 # тестируем гипотезу о равенстве
                                           # g(mu, sigma) данному значению
z <- (g_MLE_est - g_H0) / sqrt(g_cov_est)  # считаем тестовую статистику
p_value <- 2 * min(pnorm(z), 1 - pnorm(z)) # вычисляем p-value

# При помощи тестов LR, Wald и LM можно
# тестировать гипотезы следующего вида:
# H0: C(x) = q, где:
# x - вектор параметров модели: в нашем 
#     случае mu и sigma
# С(x) - вектор столбец, содержащий по строкам
#        функции от параметров модели x
# Q - вектор столбец, содержащий по строкам
#     ограничения на функций из C(x)
# r - число ограничений, совпадающее с числом строк
#     векторов столбцов C(x) и q
# Если речь идет лишь о линейных ограничениях,
# то нулевую гипотезу можно записать в виде
# H0: Rx = Q, где R это матрица с числом столбцов,
# равным числу оцениваемых параметров x
# Статистики T данных тестов имеют (в асимптотике) 
# хи-квадрат распределение, с числом степеней свободы,
# совпадающим с числом ограничений r:
# T|H0 ~ Chi2(r)
# В результате нулевая гипотеза отвергается
# на уровне значимости alpha, если:
# T > (квантиль уровня (1 - alpha) распределения Chi2(r))

# Пример №3
# H0: 3 * mu - 0.6 = sigma,
# Данную гипотезу можно переписать как:
# H0: Xi ~ N(theta, (3 * theta - 0.6) ^ 2),
# где Xi - произвольное наблюдение

# Запрограммируем функцию правдоподобия, 
# учитывающую соответствующее ограничение
lnL_R <- function(theta, X)
{
  L_vector <- dnorm(x = X,  
                    mean = theta,                   # учитываем ограничения, наложенные
                    sd = 3 * theta - 0.6)           # на параметры распределения

  lnL_value <- sum(log(L_vector))          

  return(lnL_value)                      
}
# Оценим модель при ограничении
opt_MLE_R <- optim(par = 2,                         # начальная точка
                   fn = lnL_R,                      # максимизируемая функция правдоподобия
                                                    # с заданным ограничением
                   method = "BFGS",                 # оптимизационный алгоритм
                   control = list(maxit = 1000,     # максимальное количество итераций
                                fnscale = -1,       # ставим -1 чтобы сделать задачу максимизационной
                                reltol = 1e-10),    # условие остановки алгоритма (termination condition)
                   X = X)                           # в конце задаем дополнительный аргумент для функции
# Посмотрим на полученные по результатам
# оптимизации оценки
theta_MLE <- opt_MLE_R$par                          # ММП оценка theta
mu_MLE_R <- theta_MLE                               # ММП оценка mu при заданном ограничении
sigma_MLE_R <- 3 * theta_MLE - 0.6                  # ММП оценка sigma при заданном ограничении
lnL_R_value <- opt_MLE_R$value                      # функция правдоподоюия ограниченной модели

# 1) LR тест
# Статистика:
# 2 * (lnL_F - lnL_R), где:
# lnL_F - логарифм правдоподобия 
#         полной модели
# lnL_R - логарифм правдоподобия 
#         модели с ограничениями
LR <- 2 * (lnL_value - lnL_R_value)                 # рассчитываем статистику теста
p_value_LR <- 1 - pchisq(q = LR, df = 1)            # считаем p-value
# Преимущество: очень легко считается статистика
# Недостаток:   нужно оценить и полную и
#               ограниченную модели

# Воспользуемся тестом Вальда
# Статистика: 
# W = (C(x)-q)' * (C(x) * As.Cov(x) * C(x)') * (C(x)-q), где
# при линейных ограничениях C(x) можно заменить на Rx
R <- matrix(c(3, -1), nrow = 1)                     # матрица и вектор столбец, содержащие
q <- matrix(c(0.6), ncol = 1)                       # информацию о накладываемых линейных ограничениях
                                                    # R*x=q, где x это параметры модели
x_est <- matrix(c(mu_MLE_1, sigma_MLE_1),           # вектор столбец оценок
                ncol = 1)
W <- t(R %*% x_est - q) %*%                         # вычисляем тестовую статистику
     solve(R %*% cov_MLE_1 %*% t(R)) %*% 
     (R %*% x_est - q)
p_value_W <- 1 - pchisq(q = W, df = 1)              # считаем p-value
# Преимущество: не нужно оценивать
#               ограниченную модель
# Недостаток:   нужно оценить полную модель

# Применим тест Лагранжа
# Статистика: 
# LN = grad(lnL)' * hess(lnL) * grad(lnL)
# lnL - логарифм функции правдоподобия полной
#       модели, посчитанный при ММП оценках,
#       полученных из ограниченной модели
x_est_R <- c(mu_MLE_R, sigma_MLE_R)
lnL_grad_R <- matrix(grad(func = lnL,        # используем полную функцию правдоподобия но
                                             # с использованием оценок, полученных по ограниченной
                        x = x_est_R,
                        X = X), 
                   ncol = 1)
lnL_fisher_R <- -hessian(func = lnL,         # оцениваем информацию Фишера
                          x = x_est_R,       # как умноженный на минус
                          X = X)             # единицу Гессиан
LM <- t(lnL_grad_R) %*% solve(lnL_fisher_R) %*% lnL_grad_R
p_value_LM <- 1 - pchisq(q = LM, df = 1)
# Преимущество: не нужно оценивать полную модель, но необходимо
#               запрограммировать ее функцию правдоподобия
# Недостаток:   нужно оценить ограниченную модель,
#               но обычно это гораздо проще, чем полную

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 3.1.    Протестируйте гипотезы:
#         1) H0: sigma = 6                 используя обычный метод
#         2) H0: sigma / (mu + sigma) = 1  используя дельта метод
#         3) H0: 0.2 * sigma + 2 = mu      используя LM, LR и Wald тесты
#         4*) H0: sigma / (mu + sigma) = 1 используя LM, LR и Wald тесты
#         5*) H0: 0.2 * sigma + 2 = mu     используя дельта метод
# 3.2.    Протестируйте гипотезы используя
#         тесты LM, LR и Wald:
#         1*)    H0: mu = 1
#                    sigma = 6
#         2*)    H0: mu + sigma = 10
#                    mu - sigma = 5
# 3.3.    Продолжите задание 1.3, для каждого из пунктов или для
#         указанного случая :
#         1)    Для каждого из параметров распределения в отдельности
#               проверьте гипотезу о том, что на он на 10% превышает
#               истинное значение оцениваемого параметра: например,
#               если lambda = 10, то проверьте H0: lambda = 11.
#         2)    Для распределений с двумя параметрами проверьте гипотезу
#               о том, что первый из них в два раза больше второго
#         3*)   Для двумерного нормального распределения из 8) 
#               проверьте гипотезу о независимости
#         4**)  Для двумерного нормального распределения из 8) 
#               проверьте гипотезу о том, что вероятность того, 
#               что первая из его компонент окажется больше второй,
#               равняется 0.5
#         5**)  Для двумерного нормального распределения из 8) проверьте
#               гипотезу о том, что его компоненты независимы и одинаково распределены
# 3.4.    Продолжите задание 1.4. и проверьте гипотезу о том, что:
#         1*)    H0: df1 = df2
#         2**)   H0: df1 = df2
#                    p = 0.3
#         3**)   H0: a + b = 0
#                    p * df2 = 6
# 3.5.    Продолжите задание 1.5. и протестируйте гипотезу о том, что:
#         1*)    H0: b1 = 0
#         2*)    H0: b1 = b2
#         3*)    Предельный эффект x1 на E(y|x1, x2) при x1 = 2 и x2 = 3 равен 0
#         4*)    Предельный эффект x2 на E(y|x1, x2) при x1 = 2 и x2 = 3 равен 0
#         5**)   Предельный эффект x1 и x2 на E(y|x1, x2) при x1 = 2 и x2 = 3 равны 0


#---------------------------------------------------
# Часть 4. Метод квази максимального правдоподобия,
#          устойчивый к ошибкам спецификации модели
#---------------------------------------------------

# Метод квази максимального правдоподобия позволяет
# получить состоятельные оценки параметров распределения, наиболее
# близкого к настоящему в терминах критерия Кульбака — Лейблера 

# Симулируем выборку из нецентрированного 
# распределению Стьюдента
set.seed(123)                              # в целях воспроизводимости

n <- 1000000                               # объем выборки: чем он больше, 
                                           # тем выше эффективность оценок квази
                                           # максимального правдоподобия
df <- 5                                    # математическое ожидание
ncp <- 10                                  # дисперсия
X <- rt(n = n,                             # симулируем выборку объема n 
                                           # из нормального нормального распределения
                df = df,                   # с математическим ожиданием mu
                ncp = ncp)                 # и стандартным отклонением равным

hist(X, breaks = 150)                      # графическая визуализация

# Рассчитаем реализацию оценки квази 
# максимального правдоподобия аналитически,
# используя функцию правдоподобия,
# включающую функцию плотности
# нормального распределения, а не
# центрированного Стьюдента
mean_QMLE <- mean(X)                       # для быстроты воспользуемся
sd_QMLE <- sqrt(var(X) * (n - 1) / n)      # аналитическим методом расчета

# Возникает вопрос - оценки чего мы
# получили, если настоящее распределение
# одно, а мы оценивали при допущении
# о том, что оно другое. Мы получили оценки
# параметров такого нормального распределения,
# которое является наиболее близким к настоящему
# распределению, то есть к нецентрированному
# распределению Стьюдента, по критерию
# минимизации расстояния Кульбака — Лейблера

# Расстояние Кульбака — Лейблера
# в данном случае считается как:
# E(log(f_0(Xi)) - log(f_1(Xi))), где
# Xi - произвольный элемент выборки, который
#      имеет распределение с функцией
#      плотности f_0
# f_0 - функция плотности Xi
# f_1 - функция плотности, используемая
#       при максимизации функции
#       квази максимального правдоподобия
#       с фиксированными параметрами

# Запрограммируем функцию для расчета
# расстояние Кульбака — Лейблера между
# нормальными распределением и нецентрированным
# распределением Стьюдента при фиксированных параметрах
KLIC <- function(par_norm,         # параметры нормального распределения
                 X,                # выборка из нецентрированного
                                   # распределения Стьюдента
                 mean_ln_dt_X)     # среднее значение логарифма функции
                                   # плотности нецентрированного распределения
                                   # Стьюдента по значениям из выборки X
  
{
  # Воспользуемся ЗБЧ :)
  norm_ln_den <- dnorm(X, mean = par_norm[1],
                       sd = par_norm[2], 
                       log = TRUE)            # сразу считаем логарифм
  
  value <- mean_ln_dt_X - mean(norm_ln_den)   # рассчитываем значение критерия
  
  return(value)
}

# Подготовимся к минимизации KLIC,
# создав выборку и посчитав
# E(log(f_0(Xi))) заранее, поскольку
# они не зависят от минимизируемых
# параметров, входяхих в f_1
set.seed(123)
X <- rt(1000000, df, ncp)                  # выборка из нецентрированного
mean_ln_dt_X <- mean(dt(X, df, ncp, TRUE)) # распределения Стьюдента
                                           # и рассчитанный по ней средний
                                           # логарифм функции плотности

# Минимизируем KLIC по параметрам нормального распределения
# при фиксированных на истинных значениях параметрах
# центрированного распределения Стьюдента
KLIC_opt <- optim(fn = KLIC, par = c(mean_QMLE * 0.7, # возьмем достаточно
                                     sd_QMLE * 0.7),  # отдаленные от истины точки
                  method = "BFGS",
                  control = list("maxit" = 10000,
                                 "reltol" = sqrt(.Machine$double.eps)),
                  X = X[1:100000],
                  mean_ln_dt_X = mean_ln_dt_X)

# Вывод: метод квази максимального правдоподобия
# дает состоятельные оценки параметров используемого
# для построения функции квази максимального правдоподобия
# распределения, которые (параметры) минимизируют KLIC
d <- data.frame("QMLE_max" = c(mean_QMLE, sd_QMLE), 
                "KLIC_min" = KLIC_opt$par)
rownames(d) <- c("mean", "sd")
print(d)

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 3.1*.   Повторите данный эксперимент используя логистическое
#         распределение вместо центрированного распределения Стьюдента
# 3.2**.  Повторите данный эксперимент используя логистическое
#         распределение с двумя параметрами вместо нормального распределения
# 3.3***. Повторите данный эксперимент с любыми двумя двумерными распределениями
