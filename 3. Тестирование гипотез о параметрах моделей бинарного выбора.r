# --------
# Потанин Богдан Станиславович
# Микроэконометрика в R :)
# Урок 3. Тестирование гипотез о параметрах
#         моделей бинарного выбора
# --------

# Отключим scientific notation
options(scipen = 999)

#---------------------------------------------------
# Часть 1. Тестирование гипотез о параметрах
#          с помощью LR, Wald и LM тестов
#---------------------------------------------------

# Подключим дополнительные библиотеки
library("mvtnorm")                                       # симуляции из многомерного
                                                         # нормального распределения
library("numDeriv")                                      # численное дифференцирование

# Воспроизведем процесс генерации данных,
# предполагаемый классическими моделями
# бинарного выбора с линейным индексом
# Для удобства представим, что мы симулируем процесс
# трудоустройства индивида, чтобы изучить,
# как различные характеристики могут влиять
# на вероятность занятости
# Симулируем данные
set.seed(123)                                            # для воспроизводимости
n <- 10000                                               # число наблюдений                  
X <- rmvnorm(n,                                          # симулируем n наблюдений из многомерного
                                                         # нормального распределения
             c(0, 0, 0),                                 # с нулевым вектором математических ожиданий и
             matrix(c(1, 0.2, 0.3,                       # следующей ковариационной матрице
                      0.2, 1, -0.1,
                      0.3, -0.1, 1),
                    ncol = 3,
                    byrow = FALSE))
X[, 2] <- X[, 2] > 0.5                                   # сделаем регрессор X2 бинарным
X <- cbind(1, X)                                         # добавим константу как дополнительный регрессор
mu <- 0                                                  # параметры распределения
sigma <- 7                                               # случайных ошибок
u <- rnorm(n,                                            # случайные ошибки из нормального распределения
           mu,                                           # с математическим ожиданием mu и
           sigma)                                        # стандартным отклонением sigma

# Соберем регрессоры в датафрейм
h <- data.frame("Intercept" = X[, 1],                    # константа
                "skills" = X[, 2],                       # показатель, отражающий уровень
                                                         # развития профессиональных равыков
                "male" = X[, 3],                         # пол: мужчина = 1, женщина = 0
                "experience" = X[, 4],                   # показатель, отражающий опыт работы
                "weight" = runif(n))                     # показатель веса, симулированный из
                                                         # стандартного равномерного распределения

# Создадим линейный индекс, который определяет
# зависимость латентной переменной от различных
# независимых переменных
gamma <- c(2, 3, 4, 5, -1, 1.5, 0)                       # оцениваемые регрессионные коэффициенты,
                                                         # включая константу
z_li <- gamma[1] * h$Intercept +                         # константой
        gamma[2] * h$skills +                            # навыками
        gamma[3] * h$male +                              # браком
        gamma[4] * h$experience +                        # опытом
        gamma[5] * h$experience ^ 2 +                    # квадратом опыта
        gamma[6] * h$skills * h$male +                   # взаимодействием навыков и пола
        gamma[7] * h$weight                              # но не показателем веса, поскольку
                                                         # коэффициент при нем равен нулю
z_star <- z_li  + u                                      # латентная переменная как сумма
                                                         # линейного индекса и случайной ошибки
# Чтобы оценки были достаточно точными проследим,
# чтобы дисперсии случайной ошибки и линейного
# индекса были сопоставимы
var(u)                                                   # оценка дисперсии случайной ошибки
var(z_li)                                                # оценка дисперсии линейного индекса
var(z_star)                                              # оценка дисперсии латентной переменной

# Создадим наблюдаемую зависимую переменную,
# отражающую работает индивид или нет
z <- as.numeric(z_star >= 0)                             # наблюдаемое значение переменной
z <- matrix(z, ncol = 1)                                 # как матрица с одним столбцом 
h$work <- z                                              # добавим в датафрейм переменную 
                                                         # на трудоустройство
sum(h$work  / n)                                         # доля работающих индивидов

# Примечание: рассматриваемая далее модель
# приводится исключительно в иллюстративных
# целях, поэтому предполагаемые зависимости и
# экономические интерпретации коэффициентов могут 
# не совпадать с теми, что приняты в современной
# литературе в области анализа рынка труда

#*******************************
# Пользовательский уровень
#*******************************

library("lmtest")                                                # дополнительные функции для тестирования гипотез

# Пример №1. Проверим гипотезу:
# H0: gamma experience = 0
#           experience ^ 2 = 0

# Для начала оценим параметры полной модели, не
# учитывающей ограничения, накладываемые нулевой
# гипотезой
model_probit <- glm(formula = work ~ skills + male +             # указываем формулу без константы, поскольку
                              experience + I(experience ^ 2) +   # она учитывается автоматически
                              I(skills * male) +
                              weight,                                                 
                    data = h,                                    # датафрейм, из которого берутся зависимая
                                                                 # и независимые переменные
                    family = binomial(link = "probit"))          # тип оцениваемой бинарной регрессии: в данном
                                                                 # случае используется пробит регрессия и для ее
                                                                 # замены на логит измените "probit" на "logit"

# Теперь оценим ограниченную модель, учитывая
# ограничения за счет исключения переменных
# на опыт и опыт в квадрате
model_probit_R <- glm(formula = work ~ skills + male +           # пропускаем строку 
                                                                 # experience + I(experience ^ 2)         
                                I(skills * male) +               
                                weight,                                                 
                     data = h,                                   
                                                                 
                     family = binomial(link = "probit"))         
                                                                 
                                                                 

# Посчитаем значения функций правдоподобия
lnL_F <- logLik(model_probit)                                    # полная модель
lnL_R <- logLik(model_probit_R)                                  # ограниченная модель

# Вычислим число степеней свободы
df_F <-  n - attr(lnL_F, "df")                                   # полная модель
df_R <- n - attr(lnL_R, "df")                                    # ограниченная модель

# Посмотрим на AIC
AIC_F <- AIC(model_probit)                                       # полная модель
AIC_R <- AIC(model_probit_R)                                     # ограниченная модель

# Сравним модели
RF_df <- data.frame("Full model" = c(lnL_F, df_F, AIC_F),        # аггрегируем результат
                    "Restricted model" = c(lnL_R, df_R, AIC_R))  # в датафрейме, а также
rownames(RF_df) <- c("Log-Likelihood",                           # присваиваем имена строкам
                     "Degrees of Freedom", "AIC")                # для удобства
print(RF_df)                                                     # выводим результат

# Проверим гипотезу при помощи разных тестов,
# где p-value указан как "Pr(>Chisq)"
lrtest(model_probit, model_probit_R)                             # LR тест                             
waldtest(model_probit, model_probit_R, test = "Chisq")           # Wald тест

# Пример №2. Проверим гипотезу:
# H0: gamma experience = 0.68
#           experience ^ 2 = -0.15

# Оценим ограниченную модель, учитывая ограничения за 
# счет использования фиксированных значений коэффициентов
# для опыта и опыта в квадрате, с помощью аргумента offset 
# в функции glm()
model_probit_R <- glm(formula = work ~ skills + male +              
                                I(skills * male) +                  
                                weight,       
                                data = h,
                      offset = I(0.68 * experience) +            # фиксированная часть формулы, где
                               I(-0.15 * experience ^ 2),        # коэффициенты не являются оценивамыми
                                                                 # параметрами, а задаются вручную
                      family = binomial(link = "probit"))

# Проверим гипотезу LR тестом, поскольку функция
# waldtest() не учитывает ограничения, введенные 
# за счет аргумента offset в фукнции glm()
lrtest(model_probit, model_probit_R)

# Пример №3. Проверим гипотезу: 
# H0: gamma experience = 1.5 * skills
#           experience ^ 2 = -0.15

# Оценим ограниченную модель, учитывая ограничения за 
# счет использования фиксированных значений коэффициентов
# для опыта в квадрате и рассматривая линейную комбинацию 
# от опыта и навыков как единую переменную
# Совместим оба подхода, рассмотренных ранее
model_probit_R <- glm(formula = work ~ male +              
                                I(skills * male) +                  
                                weight +   
                                I(experience + 
                                  skills / 1.5),                 # объединяем две переменных в одну
                      data = h,
                      offset = I(-0.15 * experience ^ 2),        # ограничение на коэффициент при
                                                                 # квадрате опыта
                      family = binomial(link = "probit"))

# Проверим гипотезу LR тестом, поскольку функция
# waldtest() не учитывает ограничения, введенные 
# за счет аргумента offset в фукнции glm()
lrtest(model_probit, model_probit_R)

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 1.1.    Проверьте следующие гипотезы:
#         1)    отдача от навыков (skills) не зависит от пола
#         2)    вероятность занятости не зависит от пола
#         3)    повторите предыдущий пункт предполагая, что различия
#               в отдаче любой из независимых переменных (не только skills) 
#               могут быть обусловлены полом
#         4)    вероятность занятости не зависит от пола и возрастает  
#               лишь линейно по мере набора опыта
#         5*)   отдача от навыков в два раза больше у мужчин,
#               чем у женщин
#         6**)  отдача от навыков у мужчин, во-первых, в два
#               раза больше, чем у женщин, а во-вторых совпадает 
#               с линейной отдачей от опыта
#         Примечание: для простоты под отдачей подразумевается
#                     величина коэффициента при соответствующей
#                     переменной
# 1.2.    Придумайте собственный пример с новой зависимой и 
#         независимыми переменными, в рамках которого проверяются
#         гипотезы H0:
#         1)    gamma1 = 0
#         2)    gamma1 = 2 * gamma2
#         3)    gamma1 = 2
#               gamma2 = gamma3

#*******************************
# Академический уровень
#*******************************

# Напишем функцию, позволяющую проводить 
# LR тест для двух пробит моделей
ProbitLR <- function(model_F,                                    # полная модель
                     model_R)                                    # ограниченная модель
{
  lnL_F <- logLik(model_F)
  lnL_R <- logLik(model_R)
  
  df_F <- attr(lnL_F, "df")
  df_R <- attr(lnL_R, "df")
  
  LR_stat <- 2 * (lnL_F - lnL_R)
  
  r <- df_F - df_R
  
  p_value <- 1 - pchisq(LR_stat, df = r)
  
  return_list <- list("LR_stat" = as.numeric(LR_stat),
                      "p_value" = as.numeric(p_value),
                      "r" = r)
  class(return_list) <- "ProbitLR"
  
  return(return_list)
}
# Воспользуемся функцией
ProbitLR(model_probit, model_probit_R)

# Обеспечим красивую выдачу
print.ProbitLR <- function(x)
{
  cat("Likelihood ratio test results:\n")
  
  cat("---\n")
  cat(paste("LR statistic:", x$LR_stat, "\n"))
  cat(paste("P-value:", x$p_value, "\n"))
  cat(paste("Number of restrictions:", x$r, "\n"))
}
# Вновь воспользуемся функцией
ProbitLR(model_probit, model_probit_R)

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 1.1.    Напишите функцию, позволяющую проводить LR
#         тест для различных спецификаций, написанных
#         вами ранее моделей бинарного выбора
# 1.2.    Модифицируйте функцию print.ProbitLR таким
#         образом, чтобы в консоли также отоюражались
#         1*)   переменные, на значения коэффициентов 
#               при которых накладываются ограничения
#         2**)  формулировки гипотезы H0 по аналогии
#               с тем, как это было сделано в тексте
#               примеров в пользовательском разделе
# 1.3.    Напишите модель бинарного выбора, которая
#         позволяет оценивать модель с латентной
#         переменной вида:
#         z_star = gamma1 * x1 ^ t1 + ... + gammaM * xM ^ tM + u

#---------------------------------------------------
# Часть 2. Тестирование гипотез о вероятностях
#          и предельных эффектах
#---------------------------------------------------

#*******************************
# Академический уровень
#*******************************

# Пусть имеются два индивида:
  # Борис
Boris <- data.frame("skills" = 0.5,                           # укажем характеристики
                    "male" = 1,                               # Бориса в датафрейме
                    "experience" = 0.3,
                    "weight" = 0.8)
  # Зинаида
Zina <- data.frame("skills" = 0.9,                            # укажем характеристики
                    "male" = 0,                               # Зинаиды в датафрейме
                    "experience" = 0.9,
                    "weight" = 0.1)

# Оценим для них вероятности занятости,
# используя оценки полной модели
  # P(Борис работает)
p_Boris <- predict(model_probit, 
                   type = "response", 
                   newdata = Boris)
  # P(Зинаида работает)
p_Zina <- predict(model_probit, 
                   type = "response", 
                   newdata = Zina)
  # Разница вероятностей
p_diff <- p_Boris - p_Zina

# Пример №1. Проверим гипотезу о том, что вероятности
# занятости для Бориса и Зинаиды совпадают
# H0: P(Борис работает) = P(Зина работает)
# Эквивалентная запись:
# H0: P(Борис работает) - P(Зина работает) = 0

# Будем использовать тест Вальда
# Проверяются гипотезы вида:
# H0: C(x) = 0
# Статистика теста:
# W = С(x)' * AsCov(C(x)) ^ (-1) * С(x), где:
# x - вектор параметров модели: в нашем 
#     случае gamma
# C(x) - вектор столбец из функций, накладывающих
#        ограничения на параметры
# r - число ограничений, совпадающее с числом 
#     строк вектора столбца C(x)
# При верной H0 статистика теста W имеет хи-квадрат
# распределение с r степенями свободы

# Напишем функцию, позволяющую
# дифференцировать вероятность успеха
# по коэффициентам
probGrad <- function(model,                                   # объект, возвращаемый glm() 
                     newdata = NULL)                          # датафрейм, содержащий информацию об
{                                                             # индивидах, для которых оцениваются
                                                              # вероятности
  gamma_est <- coefficients(model)                            # оценки регрессионных коэффициентов
  m <- length(gamma_est)                                      # число оцениваемых параметров

  z_li_est <- predict(model,                                  # оценка линейного индекса
                      newdata)
  z_li_d <- dnorm(z_li_est)                                   # функция плотности в точке
                                                              # линейного индекса

  if(!is.null(newdata))                                       # если не были поданы новые данные, то
  {                                                           # вероятности оцениваются для индивидов из
    model_formula <- model$formula                            # выборки, по которой оценивались параметры
    newdata[as.character(model_formula[[2]])] <- 1            # модели

    newdata <- model.frame(formula = model_formula,           # добавляем в датафрейм переменные, которые
                           data = newdata)                    # являются функциями от изначальных переменных
  }

  X <- as.matrix(newdata[, -1])                               # создаем матрицу независимых переменных,
                                                              # удаляя из нее зависимую переменную [, -1],
  X <- cbind(1, X)                                            # а затем добавляем константу    
  
  n <- nrow(X)                                                # число наблюдений
  
  my_grad <- matrix(NA, n, m)                                 # матрица, в которой по строкам будут храниться
                                                              # градиенты функции по каждой из вероятностей
  for(i in 1:m)                                               # для каждого из оцениваемых параметров
  {                                                           # рассчитываем частные производные вероятностей
    my_grad[, i] <- X[, i] * z_li_d                           # по gamma и рассчитываем значения градиента
  }
  
  colnames(my_grad) <- c("Intercept",                         # для удобства добавляем имена для
                         colnames(newdata[, -1]))             # столбцов градиента
  
  return(my_grad)                                             # возвращаем градиент
}

# Рассчитаем градиенты вероятностей занятости
# для Бориса и Зинаиды, а также для разности
# соответствующих вероятностей
pg_Boris <- probGrad(model_probit, Boris)                     # градиент по вероятности Бориса
pg_Zina <- probGrad(model_probit, Zina)                       # градиент по вероятности Зинаиды
pg_diff <- pg_Boris - pg_Zina                                 # градиент разности вероятностей
                                                              # занятости Бориса и Зинаиды как
                                                              # разность градиентов
# Расчитаем оценки асимптотической дисперсий
# разницы вероятностей занятости Бориса и Зиниды
as_cov_gamma <- vcov(model_probit)                            # достаем оценку асимптотической ковариационной
                                                              # матрицы оценок регрессионных коэффициентов
as_cov_pg_diff <- pg_diff %*% as_cov_gamma %*% t(pg_diff)     # считаем оценку асимптотической дисперсии
                                                              # оценки разности вероятностей

# Рассчитаем тестовую статистику
W_stat <- p_diff * solve(as_cov_pg_diff) * p_diff             # статистика теста Вальда
p_value <- 1 - pchisq(W_stat, df = 1)                         # p-value теста Вальда

# Пример №2. Проверим гипотезу:
# H0: P(Борис работает) = P(Зина работает)
#     P(Владимир работает) = 0.82
# Эквивалентная запись:
# H0: P(Борис работает) - P(Зина работает) = 0
#     P(Владимир работает) - 0.82 = 0

# Добавим Владимира
Vlad <- data.frame("skills" = 0.05,                          # укажем характеристики
                   "male" = 1,                               # Владимира в датафрейме
                   "experience" = 0.1,
                   "weight" = 0.1)

p_Vlad <- predict(model_probit,                              # вычисляем вероятность
                  type = "response",                         # занятости Владимира
                  newdata = Vlad)

pg_Vlad <- probGrad(model_probit, Vlad)                      # рассчитываем градиент вероятности
                                                             # занятости Владимира по оцениваемым
                                                             # параметрам

# Создадим вектор вида C(x)
p_new <- matrix(c(p_diff, p_Vlad - 0.82), ncol = 1)

# Расчитаем оценки асимптотической ковариационной
# матрицы ограничений
pg_new <- rbind(pg_diff,                                     # матрица, по строкам которой 
                pg_Vlad)                                     # расположены градиенты ограничений,
                                                             # то есть Якобиан
as_cov_pg_new <- pg_new %*% as_cov_gamma %*% t(pg_new)       # оцениваем асимптотическую 
                                                             # ковариационную матрицу ограничений

# Рассчитаем тестовую статистику
W_stat <- t(p_new) %*% solve(as_cov_pg_new) %*% p_new        # статистика теста Вальда
p_value <- 1 - pchisq(W_stat, df = 2)                        # p-value теста Вальда

# Тестирование гипотез о предельных 
# эффектах осуществляется по аналогии

# Использование теста Вальда в данном случае позволило
# нам не искать максимум функции правдоподобия при
# сложном нелинейном ограничении

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 1.1.    Проверьте гипотезу о том, что:
#         1)    вероятность занятости Бориса и Влада
#               одинакова
#         2)    вероятность занятости Бориса на 10% 
#               больше, чем у Бориса
#         3)    вероятность занятости Бориса на 10% 
#               больше, чем у Бориса, а вероятность
#               занятости Зины составляет 0.5
# 1.2.    Создайте еще двух индивидов и проверьте гипотезу
#         о том, что:
#         1*)   вероятнсоти занятости у всех пяти индивидов
#               совпадают
#         2*)   вероятность занятости у ваших двух индивидов 
#               в сумме больше, чем вероятность занятости Бориса

#---------------------------------------------------
# Часть 3. Тестирование гипотез о
#          функциональной форме
#---------------------------------------------------

#*******************************
# Академический уровень
#*******************************

# Допустим, что исследователь предполагает,
# что индекс выглядит следующим образом:
# z_li = X * gamma + t1 * exp(X * gamma) + t2 * (X * gamma) ^ 2,
# где t1 и t2 выступают в качестве дополнительных
# оцениваемых параметров
# В таком случае тест о справедливости предположения
# исследователя о том, что индекс является линейным, 
# то есть z_li = X * gamma, сводится к тесту о том, что:
# H0: t1 = 0
#     t2 = 0

# Для тестирования данной гипотезы воспользуемся
# тестом множителей Лагранжа

# Запишем функцию правдоподобия для данной модели
ProbitNonlinearLnL <- function(x, z, X)                  # функция правдоподобия
{
  gamma <- matrix(x[-c(1, 2)], ncol = 1)                 # вектор gamma коэффициентов и
  t <- matrix(x[c(1, 2)], ncol = 1)                      # вектор дополнительных параметров  
                                                         # переводим в матрицу с одним столбцом
  z_li <- X %*% gamma                                    # оценка линейного индекса
  z_est <- z_li + t[1] * exp(z_li) +                     # оценка математического ожидания 
                  t[2] * z_li ^ 2                        # латентной переменной
  
  n_obs <- nrow(X)                                       # количество наблюдений
  
  L_vec <- matrix(NA, nrow = n_obs,                      # вектор столбец вкладов наблюдений
                  ncol = 1)                              # в функцию правдоподобия
  
  is_z_0 <- z == 0                                       # вектор условий z = 0
  is_z_1 <- z == 1                                       # вектор условий z = 1
  
  L_vec[is_z_1] <- pnorm(z_est[is_z_1])                  # вклад наблюдений для которых zi = 1
  L_vec[is_z_0] <- 1 - pnorm(z_est[is_z_0])              # вклад наблюдений для которых zi = 0
  
  lnL <- sum(log(L_vec))                                 # логарифм функции правдоподобия
  
  return(lnL)
}
# Воспользуемся созданной функцией
  # Оценки модели при справедливом ограничении,
  # накладываемом нулевой гипотезой
gamma_est <- coef(model_probit)                          # достаем оценки из обычной пробит
gamma_R <- c(0, 0, gamma_est)                            # модели и приравниваем значения
names(gamma_R)[c(1, 2)] <- c("t1", "t2")                 # дополнительных параметров к значениям,
                                                         # предполагаемым нулевой гипотезой
  # Создадим матрицу регрессоров
X_mat <- as.matrix(model.frame(model_probit))            # достаем датафрейм с регрессорами и
X_mat[, 1] <- 1                                          # первращаем его в матрицу, а также
colnames(X_mat)[1] <- "Intercept"                        # заменяем зависимую переменную на константу
  # Применим функцию
lnL_R <- ProbitNonlinearLnL(gamma_R, z, X_mat)           # считаем логарифм функции правоподобия
                                                         # при ограничениях, совпадающую с логарифмом
                                                         # функции правдоподобия обычной пробит модели
lnL_R_grad <- grad(func = ProbitNonlinearLnL,            # считаем градиент данной функции
                   x = gamma_R,                          # численным методом
                   z = z, X = X_mat)
lnL_R_grad <- matrix(lnL_R_grad, ncol = 1)               # градиент как матрица с одним столбцом           
lnL_R_hess <- hessian(func = ProbitNonlinearLnL,         # считаем Гессиан данной функции
                      x = gamma_R,                       # численным методом
                      z = z, X = X_mat)
  # Реализуем тест
LM_value <- t(lnL_R_grad) %*%                            # считаем статистику теста
            solve(-lnL_R_hess) %*%                       # множителей Лагранжа
            lnL_R_grad
p_value <- 1 - pchisq(LM_value, df = 2)                  # рассчитываем p-value теста
                                                         # множителей Лагранжа

# ЗАДАНИЯ (* - непросто, ** - сложно, *** - брутально)
# 1.1.    Проверьте гипотезу о том, что:
#         1)    z_li = X * gamma + t1 * sin(X * gamma) + t2 * cos(X * gamma)
#               H0: t1 = 0
#                   t2 = 0
#         2)    z_li = X * gamma + t1 * (gamma1 + ... + gammaN + t2) ^ 2
#               H0: t1 = 0
#                   t2 = 0
#         3*)   z_li = X * gamma + t1 * (gamma1 + ... + gammaN) ^ 2
#               H0: t1 = 1
#         4*)  Повторите предыдущие пункты симулировав данные таким образом,
#              чтобы в пунктах 1) и 2) соблюдалось t1 = 1, t2 = 2, а в пункте 3) 
#              выполнялось t1 = 2
#         5**) Повторите предыдущие пункты с помощью LR теста
